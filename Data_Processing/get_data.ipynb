{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "location_file_name = '../Data/variation_locations_32k_10.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# accession_ids = open('../Data/columnIDs.txt', 'r').read().split('\\t')\n",
    "\n",
    "# Shuffle and split the accession IDs\n",
    "# train_ids, test_ids = train_test_split(accession_ids, test_size=0.2, random_state=41)\n",
    "# test_ids, val_ids = train_test_split(test_ids, test_size=0.5, random_state=41)\n",
    "\n",
    "# print(f'Train IDs: {len(train_ids)}')\n",
    "# print(f'Test IDs: {len(test_ids)}')\n",
    "# print(f'Validation IDs: {len(val_ids)}')\n",
    "\n",
    "with open(\"../Results/splits_test2.json\", \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "\n",
    "train_ids = list(map(str, splits.get(\"train\", [])))\n",
    "val_ids = list(map(str, splits.get(\"val\", [])))\n",
    "test_ids = list(map(str, splits.get(\"test\", [])))\n",
    "\n",
    "# Save the IDs as text files\n",
    "with open('../Data/train_ids.txt', 'w') as f:\n",
    "    f.write('\\t'.join(train_ids))\n",
    "\n",
    "with open('../Data/test_ids.txt', 'w') as f:\n",
    "    f.write('\\t'.join(test_ids))\n",
    "\n",
    "with open('../Data/val_ids.txt', 'w') as f:\n",
    "    f.write('\\t'.join(val_ids))\n",
    "\n",
    "\n",
    "with open('../Data/train_ids_plink.txt', 'w') as f:\n",
    "    for id in train_ids:\n",
    "        f.write(f'{id} {id}' + '\\n')\n",
    "\n",
    "with open('../Data/test_ids_plink.txt', 'w') as f:\n",
    "    for id in test_ids:\n",
    "        f.write(f'{id} {id}' + '\\n')\n",
    "\n",
    "with open('../Data/val_ids_plink.txt', 'w') as f:\n",
    "    for id in val_ids:\n",
    "        f.write(f'{id} {id}' + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original locations:\n",
      "Chromosome 1: 13 sections\n",
      "Chromosome 2: 13 sections\n",
      "Chromosome 3: 14 sections\n",
      "Chromosome 4: 13 sections\n",
      "Chromosome 5: 14 sections\n"
     ]
    }
   ],
   "source": [
    "section_locations = pickle.load(open(location_file_name, 'rb'))\n",
    "corrected_locations = {}\n",
    "\n",
    "print('Original locations:')\n",
    "for key in section_locations:\n",
    "    print(f'Chromosome {key}: {section_locations[key].shape[0]} sections')\n",
    "# Check locations for each chromosome for overlap and join\n",
    "# for i in range(1, 6):\n",
    "#     chromosome_locations = section_locations[i]\n",
    "#     j = 1\n",
    "#     while j < len(chromosome_locations):\n",
    "#         if chromosome_locations[j][0] <= chromosome_locations[j - 1][1]:\n",
    "#             # print(f'Overlap in positions {chromosome_locations[j - 1][1]} and {chromosome_locations[j][0]} on chromosome {i}')\n",
    "#             chromosome_locations[j - 1][1] = chromosome_locations[j][1]\n",
    "#             chromosome_locations = np.delete(chromosome_locations, j, axis=0)\n",
    "#             j -= 1\n",
    "#         j += 1\n",
    "\n",
    "#     corrected_locations[i] = chromosome_locations\n",
    "    \n",
    "\n",
    "\n",
    "# print('\\nCorrected locations:')\n",
    "# for key in corrected_locations:\n",
    "#     print(f'Chromosome {key}: {corrected_locations[key].shape[0]} sections')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regions\n",
    "regions = []\n",
    "for i in range(1, 6):\n",
    "    chromosome_locations = section_locations[i]\n",
    "    for j in range(len(chromosome_locations)):\n",
    "        regions.append(f'Chr{i}:{chromosome_locations[j][0]}..{chromosome_locations[j][1]}')\n",
    "\n",
    "joint_regions = \",\".join(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bioclim = pd.read_csv(\"../Data/coords_with_bioclim_30s_fixed.csv\")\n",
    "df_soil = pd.read_csv(\"../Data/coords_with_soil.csv\")\n",
    "\n",
    "# Merge on common columns, e.g., 'FID' and 'IID' if present\n",
    "merged_df = pd.merge(df_bioclim, df_soil, on=['FID', 'IID', 'LONG', 'LAT'], how='inner')\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"../Data/environmental_variables.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be normalized: ['BIO_4_7', 'BIO_10_5', 'BIO_11_6', 'BIO_12_13_16', 'BIO_14_17', 'Soil_Nitrogen_SOC', 'BIO1', 'BIO2', 'BIO3', 'BIO8', 'BIO9', 'BIO15', 'BIO18', 'BIO19', 'clay_mean', 'bdod_mean', 'wv0033_mean', 'phh2o_mean']\n",
      "Number of numeric columns: 18\n",
      "\n",
      "Normalization complete!\n",
      "Original data shape: (1095, 22)\n",
      "Normalized data shape: (1095, 22)\n",
      "\n",
      "Sample statistics after normalization:\n",
      "Mean (should be ~0):\n",
      "BIO_4_7              0.0\n",
      "BIO_10_5            -0.0\n",
      "BIO_11_6            -0.0\n",
      "BIO_12_13_16         0.0\n",
      "BIO_14_17            0.0\n",
      "Soil_Nitrogen_SOC    0.0\n",
      "BIO1                 0.0\n",
      "BIO2                 0.0\n",
      "BIO3                 0.0\n",
      "BIO8                -0.0\n",
      "BIO9                -0.0\n",
      "BIO15               -0.0\n",
      "BIO18                0.0\n",
      "BIO19                0.0\n",
      "clay_mean           -0.0\n",
      "bdod_mean            0.0\n",
      "wv0033_mean         -0.0\n",
      "phh2o_mean           0.0\n",
      "dtype: float64\n",
      "\n",
      "Std (should be ~1):\n",
      "BIO_4_7              1.000457\n",
      "BIO_10_5             1.000457\n",
      "BIO_11_6             1.000457\n",
      "BIO_12_13_16         1.000457\n",
      "BIO_14_17            1.000457\n",
      "Soil_Nitrogen_SOC    1.000457\n",
      "BIO1                 1.000457\n",
      "BIO2                 1.000457\n",
      "BIO3                 1.000457\n",
      "BIO8                 1.000457\n",
      "BIO9                 1.000457\n",
      "BIO15                1.000457\n",
      "BIO18                1.000457\n",
      "BIO19                1.000457\n",
      "clay_mean            1.000457\n",
      "bdod_mean            1.000457\n",
      "wv0033_mean          1.000457\n",
      "phh2o_mean           1.000457\n",
      "dtype: float64\n",
      "\n",
      "Scaler saved to '../Data/environmental_scaler.pkl' for inverse transformation if needed\n"
     ]
    }
   ],
   "source": [
    "# Normalize all numeric columns in the merged DataFrame\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "merged_df = pd.read_csv(\"D:\\Masters\\ReportData\\combined_variables_dataset.csv\")\n",
    "\n",
    "# Create a copy of the merged DataFrame for normalization\n",
    "normalized_df = merged_df.copy()\n",
    "\n",
    "# Identify numeric columns (exclude ID columns and coordinates)\n",
    "id_columns = ['FID', 'IID']  # These should not be normalized\n",
    "coord_columns = ['LONG', 'LAT']  # Keep coordinates as is for reference\n",
    "\n",
    "# Get all numeric columns except ID and coordinate columns\n",
    "numeric_columns = [col for col in normalized_df.columns \n",
    "                  if normalized_df[col].dtype in ['int64', 'float64'] \n",
    "                  and col not in id_columns + coord_columns]\n",
    "\n",
    "print(f\"Columns to be normalized: {numeric_columns}\")\n",
    "print(f\"Number of numeric columns: {len(numeric_columns)}\")\n",
    "\n",
    "# Initialize StandardScaler (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the numeric columns\n",
    "if numeric_columns:\n",
    "    normalized_df[numeric_columns] = scaler.fit_transform(normalized_df[numeric_columns])\n",
    "    \n",
    "    # Save the normalized DataFrame\n",
    "    normalized_df.to_csv(\"D:\\Masters\\ReportData\\combined_variables_dataset_normalized.csv\", index=False)\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(\"\\nNormalization complete!\")\n",
    "    print(f\"Original data shape: {merged_df.shape}\")\n",
    "    print(f\"Normalized data shape: {normalized_df.shape}\")\n",
    "    \n",
    "    # Show sample statistics for verification\n",
    "    print(\"\\nSample statistics after normalization:\")\n",
    "    print(\"Mean (should be ~0):\")\n",
    "    print(normalized_df[numeric_columns].mean().round(6))\n",
    "    print(\"\\nStd (should be ~1):\")\n",
    "    print(normalized_df[numeric_columns].std().round(6))\n",
    "    \n",
    "    # Save the scaler for potential inverse transformation later\n",
    "    import pickle\n",
    "    with open(\"../Data/environmental_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(\"\\nScaler saved to '../Data/environmental_scaler.pkl' for inverse transformation if needed\")\n",
    "    \n",
    "else:\n",
    "    print(\"No numeric columns found to normalize!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regions_per_chromosome(top_snps, window_size):\n",
    "    lengths = []\n",
    "    chr_snps = defaultdict(list)\n",
    "    for region in top_snps[:100]:\n",
    "        chromosome = region[0]\n",
    "        position = int(region[1])\n",
    "        chr_snps[chromosome].append(position)\n",
    "\n",
    "    regions_per_chr = {}\n",
    "    for chromosome, positions in chr_snps.items():\n",
    "        positions.sort()\n",
    "        merged = []\n",
    "        for pos in positions:\n",
    "            start = pos - window_size // 2\n",
    "            end = pos + window_size // 2\n",
    "            if not merged:\n",
    "                merged.append([start, end])\n",
    "                lengths.append(end - start)\n",
    "            else:\n",
    "                prev_start, prev_end = merged[-1]\n",
    "                if start <= prev_end + window_size // 2 and end - prev_start <= maxlen:\n",
    "                    merged[-1][1] = max(prev_end, end)\n",
    "                    lengths[-1] = merged[-1][1] - merged[-1][0]\n",
    "                else:\n",
    "                    merged.append([start, end])\n",
    "                    lengths.append(end - start)\n",
    "        regions_per_chr[chromosome] = [f'Chr{chromosome}:{start}..{end}' for start, end in merged]\n",
    "\n",
    "    print(f\"Average region length: {np.mean(lengths)}\")\n",
    "    print(f\"Max region length: {np.max(lengths)}\")\n",
    "    print(f\"Total number of regions: {sum(len(v) for v in regions_per_chr.values())}\")\n",
    "    return regions_per_chr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average region length: 1004.4183673469388\n",
      "Max region length: 1414\n",
      "Total number of regions: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 278/1135 [49:50<2:33:37, 10.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m region_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(region_chunk)\n\u001b[0;32m     23\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://tools.1001genomes.org/api/v1/pseudogenomes/strains/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/regions/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 24\u001b[0m sequence_fasta \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     25\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(sequence_fasta)\n\u001b[0;32m     26\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:1317\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno host given\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;66;03m# will parse host:port\u001b[39;00m\n\u001b[1;32m-> 1317\u001b[0m h \u001b[38;5;241m=\u001b[39m http_class(host, timeout\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttp_conn_args)\n\u001b[0;32m   1318\u001b[0m h\u001b[38;5;241m.\u001b[39mset_debuglevel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debuglevel)\n\u001b[0;32m   1320\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(req\u001b[38;5;241m.\u001b[39munredirected_hdrs)\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\http\\client.py:1422\u001b[0m, in \u001b[0;36mHTTPSConnection.__init__\u001b[1;34m(self, host, port, key_file, cert_file, timeout, source_address, context, check_hostname, blocksize)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file \u001b[38;5;241m=\u001b[39m cert_file\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1422\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mssl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_default_https_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;66;03m# send ALPN extension to indicate HTTP/1.1 protocol\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_vsn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m11\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\ssl.py:771\u001b[0m, in \u001b[0;36mcreate_default_context\u001b[1;34m(purpose, cafile, capath, cadata)\u001b[0m\n\u001b[0;32m    766\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_verify_locations(cafile, capath, cadata)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mverify_mode \u001b[38;5;241m!=\u001b[39m CERT_NONE:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;66;03m# no explicit cafile, capath or cadata but the verify mode is\u001b[39;00m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;66;03m# CERT_OPTIONAL or CERT_REQUIRED. Let's try to load default system\u001b[39;00m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;66;03m# root CA certificates for the given purpose. This may fail silently.\u001b[39;00m\n\u001b[1;32m--> 771\u001b[0m     \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_default_certs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpurpose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;66;03m# OpenSSL 1.1.1 keylog file\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(context, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeylog_filename\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\ssl.py:593\u001b[0m, in \u001b[0;36mSSLContext.load_default_certs\u001b[1;34m(self, purpose)\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m storename \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_windows_cert_stores:\n\u001b[0;32m    592\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_windows_store_certs(storename, purpose)\n\u001b[1;32m--> 593\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_default_verify_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call the API\n",
    "import time\n",
    "import urllib.request\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "top_snps = pickle.load(open('../Data/top_snps.pkl', 'rb'))\n",
    "accession_ids = open('../Data/columnIDs.txt', 'r').read().split('\\t')\n",
    "\n",
    "window_sizes = [512, 1000]\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    regions_per_chr = get_regions_per_chromosome(top_snps, window_size)\n",
    "    os.makedirs(f'../Data/Pseudogenomes/SparSNP_Joint_{window_size}', exist_ok=True)\n",
    "    for id in tqdm(accession_ids):\n",
    "        with open(f'../Data/Pseudogenomes/SparSNP_Joint_{window_size}/{id}.fa', 'wb') as f:\n",
    "            for chromosome, regions in regions_per_chr.items():\n",
    "                # Process 5 regions at a time\n",
    "                for i in range(0, len(regions), 4):\n",
    "                    region_chunk = regions[i:i+4]\n",
    "                    region_str = \",\".join(region_chunk)\n",
    "                    url = f'https://tools.1001genomes.org/api/v1/pseudogenomes/strains/{id}/regions/{region_str}'\n",
    "                    sequence_fasta = urllib.request.urlopen(url).read()\n",
    "                    f.write(sequence_fasta)\n",
    "                    f.write(b'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show where the SNPs are located in the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 131072\n",
    "\n",
    "chromosome_sizes = {\n",
    "    1: 30427671,\n",
    "    2: 19698289,\n",
    "    3: 23459830,\n",
    "    4: 18585056,\n",
    "    5: 26975502,\n",
    "}\n",
    "\n",
    "def get_regions(top_snps, window_size, merge_regions=True):\n",
    "    lengths = []\n",
    "    maxlen = window_size\n",
    "    # Group SNPs by chromosome\n",
    "    chr_snps = defaultdict(list)\n",
    "    for region in top_snps:\n",
    "        chromosome = region[0]\n",
    "        position = int(region[1])\n",
    "        chr_snps[chromosome].append(position)\n",
    "\n",
    "    regions = []\n",
    "    for chromosome, positions in chr_snps.items():\n",
    "        positions.sort()\n",
    "        merged = []\n",
    "        for pos in positions:\n",
    "            # Center the window on the SNP, but ensure the region is exactly maxlen\n",
    "            start = pos - maxlen // 2\n",
    "            end = start + maxlen\n",
    "            # Ensure start and end are within chromosome size limits\n",
    "            if start < 0:\n",
    "                start = 0\n",
    "                end = maxlen\n",
    "            if end > chromosome_sizes[chromosome]:\n",
    "                end = chromosome_sizes[chromosome]\n",
    "                start = end - maxlen\n",
    "                if start < 0:\n",
    "                    start = 0\n",
    "            \n",
    "            if not merged or not merge_regions:\n",
    "                # Either first region or merging is disabled\n",
    "                merged.append([start, end])\n",
    "                lengths.append(end - start)\n",
    "            else:\n",
    "                prev_start, prev_end = merged[-1]\n",
    "                # If current start is less than or equal to previous end, merge\n",
    "                if start <= prev_end:\n",
    "                    # Merge by extending the previous region to cover the new region\n",
    "                    new_end = max(prev_end, end)\n",
    "                    new_start = prev_start\n",
    "                    \n",
    "                    # Check if the merged region exceeds maxlen\n",
    "                    if new_end - new_start > maxlen:\n",
    "                        # Need to choose which SNPs to prioritize\n",
    "                        # Option 1: Keep the region at maxlen starting from the first SNP\n",
    "                        # This might exclude the current SNP\n",
    "                        if new_start + maxlen < pos + maxlen // 2:\n",
    "                            # Current SNP would be too far from center, start new region\n",
    "                            merged.append([start, end])\n",
    "                            lengths.append(end - start)\n",
    "                        else:\n",
    "                            # Trim to maxlen, keeping first SNP well covered\n",
    "                            merged[-1][1] = new_start + maxlen\n",
    "                            lengths[-1] = maxlen\n",
    "                    else:\n",
    "                        # Merged region is within maxlen, update it\n",
    "                        merged[-1][1] = new_end\n",
    "                        lengths[-1] = new_end - new_start\n",
    "                else:\n",
    "                    merged.append([start, end])\n",
    "                    lengths.append(end - start)\n",
    "        \n",
    "        # Add merged regions to the list, ensuring each is exactly maxlen\n",
    "        for start, end in merged:\n",
    "            regions.append(f'Chr{chromosome}:{int(start)}..{int(end)}')\n",
    "\n",
    "    joint_regions = \",\".join(regions)\n",
    "    merge_status = \"with merging\" if merge_regions else \"without merging\"\n",
    "    print(f\"Regions created {merge_status}\")\n",
    "    print(f\"Average region length: {np.mean(lengths)}\")\n",
    "    print(f\"Max region length: {np.max(lengths)}\")\n",
    "    print(f\"Total number of regions: {len(regions)}\")\n",
    "    return joint_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions created without merging\n",
      "Average region length: 512.0\n",
      "Max region length: 512\n",
      "Total number of regions: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1135 [00:00<02:02,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 88: HTTP Error 403: Forbidden\n",
      "Error downloading 108: HTTP Error 403: Forbidden\n",
      "Error downloading 139: HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1135 [00:00<01:42, 11.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 159: HTTP Error 403: Forbidden\n",
      "Error downloading 265: HTTP Error 403: Forbidden\n",
      "Error downloading 350: HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1135 [00:00<01:31, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 351: HTTP Error 403: Forbidden\n",
      "Error downloading 403: HTTP Error 403: Forbidden\n",
      "Error downloading 410: HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1135 [00:00<01:28, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 424: HTTP Error 403: Forbidden\n",
      "Error downloading 428: HTTP Error 403: Forbidden\n",
      "Error downloading 430: HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 15/1135 [00:01<01:25, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 470: HTTP Error 403: Forbidden\n",
      "Error downloading 476: HTTP Error 403: Forbidden\n",
      "Error downloading 484: HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 17/1135 [00:01<01:24, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 504: HTTP Error 403: Forbidden\n",
      "Error downloading 506: HTTP Error 403: Forbidden\n",
      "Error downloading 531: HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1135 [00:01<01:21, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 544: HTTP Error 403: Forbidden\n",
      "Error downloading 546: HTTP Error 403: Forbidden\n",
      "Error downloading 628: HTTP Error 403: Forbidden\n",
      "Error downloading 630: HTTP Error 403: Forbidden\n",
      "Error downloading 630: HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 25/1135 [00:02<01:49, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 680: HTTP Error 403: Forbidden\n",
      "Error downloading 681: HTTP Error 403: Forbidden\n",
      "Error downloading 685: HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 27/1135 [00:02<01:41, 10.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 687: HTTP Error 403: Forbidden\n",
      "Error downloading 728: HTTP Error 403: Forbidden\n",
      "Error downloading 742: HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/1135 [00:02<01:36, 11.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading 763: HTTP Error 403: Forbidden\n",
      "Error downloading 765: HTTP Error 403: Forbidden\n",
      "Error downloading 766: HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://tools.1001genomes.org/api/v1/pseudogenomes/strains/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/regions/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjoint_regions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Use the SSL context that doesn't verify certificates\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     sequence_fasta \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data/Pseudogenomes/SparSNP_FixedLen_200_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.fa\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     29\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(sequence_fasta)\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\urllib\\request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m         \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\http\\client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1281\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1283\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\http\\client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\http\\client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\http\\client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1036\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1041\u001b[0m \n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\http\\client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 976\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\http\\client.py:1455\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1453\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m-> 1455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    508\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    510\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   1102\u001b[0m             \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\andro\\anaconda3\\envs\\Masters\\lib\\ssl.py:1375\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[0;32m   1374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1375\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call the API\n",
    "import time\n",
    "import pickle\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "# Create an SSL context that doesn't verify certificates (use with caution)\n",
    "ssl_context = ssl.create_default_context()\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "top_snps = pickle.load(open('../Data/Pseudogenomes/top_snps_200.pkl', 'rb'))\n",
    "accession_ids = open('../Data/columnIDs.txt', 'r').read().split('\\t')\n",
    "\n",
    "window_sizes = [512, 1000]\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    # Set merge_regions=False to prevent regions from merging\n",
    "    joint_regions = get_regions(top_snps, window_size, merge_regions=False)\n",
    "    os.makedirs(f'../Data/Pseudogenomes/SparSNP_FixedLen_{window_size}', exist_ok=True)\n",
    "    for id in tqdm(accession_ids):\n",
    "        url = f'https://tools.1001genomes.org/api/v1/pseudogenomes/strains/{id}/regions/{joint_regions}'\n",
    "        try:\n",
    "            # Use the SSL context that doesn't verify certificates\n",
    "            sequence_fasta = urllib.request.urlopen(url, context=ssl_context).read()\n",
    "            with open(f'../Data/Pseudogenomes/SparSNP_FixedLen_200_{window_size}/{id}.fa', 'wb') as f:\n",
    "                f.write(sequence_fasta)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    #API is limited to 2 requests per second, be safe\n",
    "    # time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 1_000_000\n",
    "\n",
    "chromosome_sizes = {\n",
    "    1: 30427671,\n",
    "    2: 19698289,\n",
    "    3: 23459830,\n",
    "    4: 18585056,\n",
    "    5: 26975502,\n",
    "}\n",
    "\n",
    "\n",
    "def get_regions_batched(top_snps, window_size, max_snps_per_batch=100, merge_regions=True):\n",
    "    \"\"\"\n",
    "    Create regions from SNPs, splitting into batches if there are too many SNPs.\n",
    "    Returns a list of region strings, one per batch.\n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "    maxlen = window_size\n",
    "    \n",
    "    # Limit the number of SNPs if there are too many\n",
    "    if len(top_snps) > max_snps_per_batch:\n",
    "        print(f\"Too many SNPs ({len(top_snps)}), splitting into batches of {max_snps_per_batch}\")\n",
    "        \n",
    "        # Split SNPs into batches\n",
    "        snp_batches = []\n",
    "        for i in range(0, len(top_snps), max_snps_per_batch):\n",
    "            snp_batches.append(top_snps[i:i + max_snps_per_batch])\n",
    "    else:\n",
    "        snp_batches = [top_snps]\n",
    "    \n",
    "    all_region_strings = []\n",
    "    \n",
    "    for batch_idx, snp_batch in enumerate(snp_batches):\n",
    "        print(f\"Processing batch {batch_idx + 1}/{len(snp_batches)} with {len(snp_batch)} SNPs\")\n",
    "        \n",
    "        # Group SNPs by chromosome for this batch\n",
    "        chr_snps = defaultdict(list)\n",
    "        for region in snp_batch:\n",
    "            chromosome = region[0]\n",
    "            position = int(region[1])\n",
    "            chr_snps[chromosome].append(position)\n",
    "\n",
    "        regions = []\n",
    "        batch_lengths = []\n",
    "        \n",
    "        for chromosome, positions in chr_snps.items():\n",
    "            positions.sort()\n",
    "            used_positions = []\n",
    "            \n",
    "            for pos in positions:\n",
    "                # Check if this SNP would fall within the window of any previously used SNP\n",
    "                skip_snp = False\n",
    "                for used_pos in used_positions:\n",
    "                    # Calculate the distance between SNPs\n",
    "                    if abs(pos - used_pos) < maxlen:\n",
    "                        skip_snp = True\n",
    "                        break\n",
    "                \n",
    "                if skip_snp:\n",
    "                    continue\n",
    "                \n",
    "                # Center the window on the SNP, but ensure the region is exactly maxlen\n",
    "                start = pos - maxlen // 2\n",
    "                end = start + maxlen\n",
    "                # Ensure start and end are within chromosome size limits\n",
    "                if start < 0:\n",
    "                    start = 0\n",
    "                    end = maxlen\n",
    "                if end > chromosome_sizes[chromosome]:\n",
    "                    end = chromosome_sizes[chromosome]\n",
    "                    start = end - maxlen\n",
    "                    if start < 0:\n",
    "                        start = 0\n",
    "                \n",
    "                # Add this region and mark the SNP position as used\n",
    "                regions.append(f'Chr{chromosome}:{int(start)}..{int(end)}')\n",
    "                batch_lengths.append(end - start)\n",
    "                used_positions.append(pos)\n",
    "\n",
    "        joint_regions = \",\".join(regions)\n",
    "        all_region_strings.append(joint_regions)\n",
    "        lengths.extend(batch_lengths)\n",
    "        \n",
    "        print(f\"Batch {batch_idx + 1} regions created without overlapping windows\")\n",
    "        print(f\"Batch {batch_idx + 1} regions: {len(regions)}\")\n",
    "\n",
    "    print(f\"Total average region length: {np.mean(lengths)}\")\n",
    "    print(f\"Total max region length: {np.max(lengths)}\")\n",
    "    print(f\"Total number of regions across all batches: {len(lengths)}\")\n",
    "    print(f\"Number of batches: {len(all_region_strings)}\")\n",
    "    \n",
    "    return all_region_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted 500 SNPs by chromosome and position\n",
      "Too many SNPs (500), splitting into batches of 100\n",
      "Processing batch 1/5 with 100 SNPs\n",
      "Batch 1 regions created without overlapping windows\n",
      "Batch 1 regions: 97\n",
      "Processing batch 2/5 with 100 SNPs\n",
      "Batch 2 regions created without overlapping windows\n",
      "Batch 2 regions: 99\n",
      "Processing batch 3/5 with 100 SNPs\n",
      "Batch 3 regions created without overlapping windows\n",
      "Batch 3 regions: 100\n",
      "Processing batch 4/5 with 100 SNPs\n",
      "Batch 4 regions created without overlapping windows\n",
      "Batch 4 regions: 99\n",
      "Processing batch 5/5 with 100 SNPs\n",
      "Batch 5 regions created without overlapping windows\n",
      "Batch 5 regions: 97\n",
      "Total average region length: 512.0\n",
      "Total max region length: 512\n",
      "Total number of regions across all batches: 492\n",
      "Number of batches: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing accessions for window size 512: 100%|██████████| 1135/1135 [30:55<00:00,  1.63s/it]\n",
      "Processing accessions for window size 512: 100%|██████████| 1135/1135 [30:55<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "# Create an SSL context that doesn't verify certificates (use with caution)\n",
    "ssl_context = ssl.create_default_context()\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "top_snps = pickle.load(open('../Data/Pseudogenomes/top_snps_500.pkl', 'rb'))\n",
    "\n",
    "# Sort SNPs by chromosome first, then by position\n",
    "top_snps_sorted = sorted(top_snps, key=lambda x: (x[0], int(x[1])))\n",
    "print(f\"Sorted {len(top_snps_sorted)} SNPs by chromosome and position\")\n",
    "\n",
    "accession_ids = open('../Data/columnIDs.txt', 'r').read().split('\\t')\n",
    "\n",
    "window_sizes = [512]\n",
    "max_snps_per_request = 100  # Adjust this based on what works for your API\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    # Get batched region strings\n",
    "    region_batches = get_regions_batched(top_snps_sorted, window_size, max_snps_per_request, merge_regions=False)\n",
    "    \n",
    "    os.makedirs(f'../Data/Pseudogenomes/SparSNP_FixedLen_500_{window_size}', exist_ok=True)\n",
    "    \n",
    "    for id in tqdm(accession_ids, desc=f\"Processing accessions for window size {window_size}\"):\n",
    "        all_sequences = []\n",
    "        \n",
    "        # Process each batch of regions\n",
    "        for batch_idx, joint_regions in enumerate(region_batches):\n",
    "            url = f'https://tools.1001genomes.org/api/v1/pseudogenomes/strains/{id}/regions/{joint_regions}'\n",
    "            try:\n",
    "                # Use the SSL context that doesn't verify certificates\n",
    "                sequence_fasta = urllib.request.urlopen(url, context=ssl_context).read()\n",
    "                all_sequences.append(sequence_fasta)\n",
    "                \n",
    "                # Be nice to the API - small delay between batch requests\n",
    "                if len(region_batches) > 1:\n",
    "                    time.sleep(0.1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading batch {batch_idx + 1} for {id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all sequences and write to file\n",
    "        if all_sequences:\n",
    "            with open(f'../Data/Pseudogenomes/SparSNP_FixedLen_500_{window_size}/{id}.fa', 'wb') as f:\n",
    "                for seq in all_sequences:\n",
    "                    f.write(seq)\n",
    "                    # Add newline between batches if needed\n",
    "                    if not seq.endswith(b'\\n'):\n",
    "                        f.write(b'\\n')\n",
    "        \n",
    "        # API rate limiting\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Using requests library (more robust)\n",
    "# First install: pip install requests\n",
    "# import requests\n",
    "\n",
    "# for id in tqdm(accession_ids):\n",
    "#     url = f'https://tools.1001genomes.org/api/v1/pseudogenomes/strains/{id}/regions/{joint_regions}'\n",
    "#     try:\n",
    "#         response = requests.get(url, verify=False)  # verify=False disables SSL verification\n",
    "#         response.raise_for_status()\n",
    "#         with open(f'../Data/Pseudogenomes/SparSNP_FixedLen_{window_size}/{id}.fa', 'wb') as f:\n",
    "#             f.write(response.content)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error downloading {id}: {e}\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [01:40<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Call the SoilGrids API\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "coverage = \"phh2o_5-15cm_uncertainty\"\n",
    "\n",
    "os.makedirs(f'../Data/SoilGrids/{coverage}', exist_ok=True)\n",
    "\n",
    "accession_ids = open('../Data/columnIDs.txt', 'r').read().split('\\t')\n",
    "sample_map_array = np.ndarray((0, 2))\n",
    "\n",
    "areas = pickle.load(open('../Data/sample_areas.pkl', 'rb'))\n",
    "\n",
    "for region in tqdm(areas):\n",
    "\n",
    "    min_long = region[\"min_long\"]\n",
    "    max_long = region[\"max_long\"]\n",
    "    min_lat = region[\"min_lat\"]\n",
    "    max_lat = region[\"max_lat\"]\n",
    "\n",
    "    url = f'https://maps.isric.org/mapserv?map=/map/{coverage.split(\"_\")[0]}.map&SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCoverage&COVERAGEID={coverage}&FORMAT=image/tiff&SUBSET=long({min_long:.4f},{max_long:.4f})&SUBSET=lat({min_lat:.4f},{max_lat:.4f})&SUBSETTINGCRS=http://www.opengis.net/def/crs/EPSG/0/4326&OUTPUTCRS=http://www.opengis.net/def/crs/EPSG/0/4326'\n",
    "    tif_file = urllib.request.urlopen(url).read()\n",
    "    file_name = f'../Data/SoilGrids/{coverage}/{coverage}_{int(min_long)}_{int(max_long)}_{int(min_lat)}_{int(max_lat)}.tif'\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(tif_file)\n",
    "    for key in region[\"ids\"]:\n",
    "        sample_map_array = np.vstack((sample_map_array, [key, file_name]))\n",
    "\n",
    "pickle.dump(sample_map_array, open(f'../Data/SoilGrids/{coverage}/sample_image_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bioclim_file = \"../Data/coords_with_bioclim_30s.csv\"\n",
    "bioclim = pd.read_csv(bioclim_file)\n",
    "\n",
    "filename = \"../Data/bioclim_6.fam\"\n",
    "\n",
    "with open(filename, \"w+\") as f:\n",
    "    for index, row in bioclim.iterrows():\n",
    "\n",
    "        f.write(f\"{row['FID']:.0f} {row['IID']:.0f} 0 0 0 {row['BIO6']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_file = \"../Data/missing_data.txt\"\n",
    "filename = \"../Data/plink.fam\"\n",
    "with open(filename, 'r') as f:\n",
    "    with open(missing_file, 'w') as m:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.strip().split(\" \")[5] == \"-3.4e+38\":\n",
    "                m.write(f\"{line.split(' ')[0]} {line.split(' ')[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_file = \"../Data/plink_bio6_train.fam\"\n",
    "test_file = \"../Data/plink_bio6_test.fam\"\n",
    "\n",
    "csv = pd.read_csv(fam_file, sep=\" \", header=None)\n",
    "csv_test = pd.read_csv(test_file, sep=\" \", header=None)\n",
    "\n",
    "mean = csv[5].mean()\n",
    "std = csv[5].std()\n",
    "\n",
    "csv[5] = (csv[5] - mean) / std\n",
    "csv_test[5] = (csv_test[5] - mean) / std\n",
    "\n",
    "csv.to_csv(fam_file, sep=\" \", header=False, index=False)\n",
    "csv_test.to_csv(test_file, sep=\" \", header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.821640091116173 5.467024484498266\n"
     ]
    }
   ],
   "source": [
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bim_file = \"../Data/plink.bim\"\n",
    "out_file = \"../Data/renamed.bim\"\n",
    "\n",
    "with open(bim_file, 'r') as f:\n",
    "    with open(out_file, 'w') as o:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            split = line.split('\\t')\n",
    "            split[1] = f\"{split[0]}_{split[3]}\"\n",
    "            o.write('\\t'.join(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat embeddings\n",
    "folder_name = '../Data/Embeddings/Variance/'\n",
    "import pickle\n",
    "\n",
    "\n",
    "embeddings = {}\n",
    "for id in accession_ids:\n",
    "    embed = pickle.load(open(f'{folder_name}{id}.embed', 'rb'))\n",
    "    embeddings[id] = embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(embeddings, open(f'../Data/Embeddings/Variance-Embeddings.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
